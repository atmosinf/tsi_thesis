{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5e3280b",
   "metadata": {},
   "source": [
    "### in this notebook, we'll try to use our model in a video stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1edd4062",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3da9d745",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc095ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data import random_split\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2230f993",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vgg(nn.Module):\n",
    "    def __init__(self, drop=0.2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1a = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.conv1b = nn.Conv2d(64, out_channels=64, kernel_size=3, padding=1)\n",
    "\n",
    "        self.conv2a = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.conv2b = nn.Conv2d(128, 128, 3, padding=1)\n",
    "\n",
    "        self.conv3a = nn.Conv2d(128, 256, 3, padding=1)\n",
    "        self.conv3b = nn.Conv2d(256, 256, 3, padding=1)\n",
    "\n",
    "        self.conv4a = nn.Conv2d(256, 512, 3, padding=1)\n",
    "        self.conv4b = nn.Conv2d(512, 512, 3, padding=1)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.bn1a = nn.BatchNorm2d(64)\n",
    "        self.bn1b = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.bn2a = nn.BatchNorm2d(128)\n",
    "        self.bn2b = nn.BatchNorm2d(128)\n",
    "\n",
    "        self.bn3a = nn.BatchNorm2d(256)\n",
    "        self.bn3b = nn.BatchNorm2d(256)\n",
    "\n",
    "        self.bn4a = nn.BatchNorm2d(512)\n",
    "        self.bn4b = nn.BatchNorm2d(512)\n",
    "\n",
    "        # self.lin1 = nn.Linear(512 * 2 * 2, 4096)\n",
    "        self.lin1 = nn.Linear(512 * 3 * 3, 4096)\n",
    "        self.lin2 = nn.Linear(4096, 4096)\n",
    "\n",
    "        # self.lin3 = nn.Linear(4096, 7)\n",
    "        # output size changed to 9 because we're using the fer plus labels, not the fer 2013 labels\n",
    "        self.lin3 = nn.Linear(4096, 9)\n",
    "\n",
    "        self.drop = nn.Dropout(p=drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1a(self.conv1a(x)))\n",
    "        x = F.relu(self.bn1b(self.conv1b(x)))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = F.relu(self.bn2a(self.conv2a(x)))\n",
    "        x = F.relu(self.bn2b(self.conv2b(x)))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = F.relu(self.bn3a(self.conv3a(x)))\n",
    "        x = F.relu(self.bn3b(self.conv3b(x)))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = F.relu(self.bn4a(self.conv4a(x)))\n",
    "        x = F.relu(self.bn4b(self.conv4b(x)))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        # x = x.view(-1, 512 * 2 * 2)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.drop(self.lin1(x)))\n",
    "        x = F.relu(self.drop(self.lin2(x)))\n",
    "        x = self.lin3(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2738aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Vgg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5798b5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "547d570b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('saved_model/vggmodel_state_dict.pth', map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33f3a514",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToPILImage(),\n",
    "                            transforms.Resize((48,48)),\n",
    "                            transforms.Grayscale(),\n",
    "                            transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492f42cd",
   "metadata": {},
   "source": [
    "### create a function that accepts an image and reurns the prediction and the label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0d4f585e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(img):\n",
    "    labels = ['neutral', 'happiness', 'surprise', 'sadness', 'anger', 'disgust', 'fear', 'contempt', 'unknown']\n",
    "    img_transformed = transform(img)\n",
    "    img_transformed.unsqueeze_(1)\n",
    "    pred = model(img_transformed)\n",
    "    label = labels[torch.argmax(pred)]\n",
    "    pred = pred.detach().numpy()[0]\n",
    "    \n",
    "    return pred, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "70a76db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = np.array([0.613, 1.342, 2.134, -1.435, -2.455, 1.535, 0.257, 0.893, -1.678])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ba07be79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.455"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max([np.max(pred), np.min(pred)*-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3df836c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "height = 500\n",
    "width = 500\n",
    "blank = np.zeros((height,width,3), dtype='uint8')\n",
    "maxheight = int(height/2.1)\n",
    "origin = int(height/2)\n",
    "binwidth = int(width/10)\n",
    "# maxheight = int(500*(np.max([np.max(pred), np.min(pred)*-1])))\n",
    "for i,item in enumerate(pred):\n",
    "    i += 1\n",
    "    pt1 = (int(binwidth*(i-0.5)),origin)\n",
    "    pt2 = (int(binwidth*(i+0.5)),int(origin-(maxheight*(item/np.max([np.max(pred), np.min(pred)*-1])))))\n",
    "    cv.rectangle(blank, pt1, pt2, (0,255,0), thickness=-1)\n",
    "cv.imshow('rectangle', blank)\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()\n",
    "cv.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c62ec2",
   "metadata": {},
   "source": [
    "### create a function that graphs the predicted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ef62d2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_pred(pred):\n",
    "    height = 500\n",
    "    width = 500\n",
    "    blank = np.zeros((height,width,3), dtype='uint8')\n",
    "    maxheight = int(height/2.3)\n",
    "    origin = int(height/2)\n",
    "    binwidth = int(width/10)\n",
    "    for i,item in enumerate(pred):\n",
    "        i += 1\n",
    "        pt1 = (int(binwidth*(i-0.5)),origin)\n",
    "        pt2 = (int(binwidth*(i+0.5)),int(origin-(maxheight*(item/np.max([np.max(pred), np.min(pred)*-1])))))\n",
    "        cv.rectangle(blank, pt1, pt2, (0,255,0), thickness=-1)\n",
    "    labels = ['neutral', 'happiness', 'surprise', 'sadness', 'anger', 'disgust', 'fear', 'contempt', 'unknown']\n",
    "    cv.putText(blank, labels[0], (int(binwidth*(1-0.5)),height-10), cv.FONT_HERSHEY_SIMPLEX, 0.3, (0, 255, 0), 2, cv.LINE_AA)\n",
    "    cv.putText(blank, labels[1], (int(binwidth*(2-0.5)),height-10), cv.FONT_HERSHEY_SIMPLEX, 0.3, (0, 255, 0), 2, cv.LINE_AA)\n",
    "    cv.putText(blank, labels[2], (int(binwidth*(3-0.5)),height-10), cv.FONT_HERSHEY_SIMPLEX, 0.3, (0, 255, 0), 2, cv.LINE_AA)\n",
    "    cv.putText(blank, labels[3], (int(binwidth*(4-0.5)),height-10), cv.FONT_HERSHEY_SIMPLEX, 0.3, (0, 255, 0), 2, cv.LINE_AA)\n",
    "    cv.putText(blank, labels[4], (int(binwidth*(5-0.5)),height-10), cv.FONT_HERSHEY_SIMPLEX, 0.3, (0, 255, 0), 2, cv.LINE_AA)\n",
    "    cv.putText(blank, labels[5], (int(binwidth*(6-0.5)),height-10), cv.FONT_HERSHEY_SIMPLEX, 0.3, (0, 255, 0), 2, cv.LINE_AA)\n",
    "    cv.putText(blank, labels[6], (int(binwidth*(7-0.5)),height-10), cv.FONT_HERSHEY_SIMPLEX, 0.3, (0, 255, 0), 2, cv.LINE_AA)\n",
    "    cv.putText(blank, labels[7], (int(binwidth*(8-0.5)),height-10), cv.FONT_HERSHEY_SIMPLEX, 0.3, (0, 255, 0), 2, cv.LINE_AA)\n",
    "    cv.putText(blank, labels[8], (int(binwidth*(9-0.5)),height-10), cv.FONT_HERSHEY_SIMPLEX, 0.3, (0, 255, 0), 2, cv.LINE_AA)\n",
    "    cv.imshow('rectangle', blank)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4616225",
   "metadata": {},
   "source": [
    "### capture the webcam video and display the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4d6851ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv.VideoCapture(0)\n",
    "shapelist = []\n",
    "if not cap.isOpened():\n",
    "    print(\"Cannot open camera\")\n",
    "    exit()\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "    # if frame is read correctly ret is True\n",
    "    if not ret:\n",
    "        print(\"Can't receive frame (stream end?). Exiting ...\")\n",
    "        break\n",
    "    # Our operations on the frame come here\n",
    "#     gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)\n",
    "    img = frame\n",
    "    img = cv.flip(img, 1)\n",
    "    pred, label = predict(img)\n",
    "    \n",
    "    predtext = np.array2string(pred)\n",
    "        \n",
    "    cv.putText(img, label, (10,450), cv.FONT_HERSHEY_SIMPLEX, 3, (0, 255, 0), 2, cv.LINE_AA)\n",
    "    # Display the resulting frameq\n",
    "    cv.imshow('frame', img)\n",
    "    \n",
    "    graph_pred(pred)\n",
    "    \n",
    "#     graph = np.ones((300,800,1))\n",
    "#     cv.putText(graph, predtext, (10,300-10), cv.FONT_HERSHEY_SIMPLEX, 0.35, (0, 255, 0), 2, cv.LINE_AA)\n",
    "#     cv.imshow('pred', graph)\n",
    "    \n",
    "    if cv.waitKey(1) == ord('q'):\n",
    "        break\n",
    "    \n",
    "    shapelist.append(frame.shape)\n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5619ba1e",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1829472c",
   "metadata": {},
   "source": [
    "An app was created that uses opencv and a PyTorch NN model, that takes in a video feed from the webcam and outputs the detected emotion and the predicted value for each emotion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d459aa5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "udacitycvenv",
   "language": "python",
   "name": "udacitycvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
